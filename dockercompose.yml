version: '3.8'

services:
  # ============================================
  # SINGLE POSTGRESQL INSTANCE (All Databases)
  # ============================================
  
  postgres:
    image: postgres:15
    container_name: poc-postgres
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres123
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./init-scripts:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - poc-network

  # ============================================
  # STORAGE LAYER (S3-Compatible)
  # ============================================
  
  minio:
    image: minio/minio:latest
    container_name: poc-minio
    ports:
      - "9000:9000"  # S3 API
      - "9001:9001"  # Web UI
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: password123
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - poc-network

  # MinIO bucket initialization
  minio-setup:
    image: minio/mc:latest
    container_name: poc-minio-setup
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set myminio http://minio:9000 admin password123;
      mc mb myminio/tenant-a-bucket --ignore-existing;
      mc mb myminio/tenant-b-bucket --ignore-existing;
      mc mb myminio/warehouse --ignore-existing;
      mc policy set download myminio/tenant-a-bucket;
      mc policy set download myminio/tenant-b-bucket;
      mc policy set download myminio/warehouse;
      echo 'Buckets created: tenant-a-bucket, tenant-b-bucket, warehouse';
      exit 0;
      "
    networks:
      - poc-network

  # ============================================
  # POLARIS CATALOG (Iceberg Backend)
  # ============================================
  
  polaris:
    image: apache/polaris:latest
    container_name: poc-polaris
    ports:
      - "8181:8181"  # REST Catalog API
      - "8182:8182"  # Admin API
    environment:
      # PostgreSQL connection
      POLARIS_PERSISTENCE_TYPE: eclipse-link
      
      # S3 Configuration
      AWS_ACCESS_KEY_ID: admin
      AWS_SECRET_ACCESS_KEY: password123
      AWS_REGION: us-east-1
      S3_ENDPOINT: http://minio:9000
      
      # Polaris configuration
      POLARIS_BOOTSTRAP: "true"
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    volumes:
      - ./polaris-server.yml:/app/config/polaris-server.yml
    command: ["server", "/app/config/polaris-server.yml"]
    networks:
      - poc-network

  # ============================================
  # HIVE METASTORE (HMS - Legacy Backend)
  # ============================================
  
  hive-metastore:
    image: apache/hive:4.0.0
    container_name: poc-hive-metastore
    ports:
      - "9083:9083"  # Thrift API
    environment:
      SERVICE_NAME: metastore
      DB_DRIVER: postgres
      
      # PostgreSQL connection
      SERVICE_OPTS: >-
        -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver
        -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore_db
        -Djavax.jdo.option.ConnectionUserName=hive
        -Djavax.jdo.option.ConnectionPassword=hive123
        -Dhive.metastore.warehouse.dir=s3a://warehouse/
        -Dhive.metastore.schema.verification=false
        -Dhadoop.proxyuser.hive.hosts=*
        -Dhadoop.proxyuser.hive.groups=*
      
      # S3 Configuration
      AWS_ACCESS_KEY_ID: admin
      AWS_SECRET_ACCESS_KEY: password123
      AWS_REGION: us-east-1
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    volumes:
      - ./hive-site.xml:/opt/hive/conf/hive-site.xml
      - ./core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
    networks:
      - poc-network

  # ============================================
  # GRAVITINO (Unified Catalog Layer)
  # ============================================
  
  gravitino:
    image: datastrato/gravitino:0.6.0
    container_name: poc-gravitino
    ports:
      - "8090:8090"  # REST API
      - "8091:8091"  # Web UI (if available)
    environment:
      # PostgreSQL backend
      GRAVITINO_METALAKE_NAME: bank_platform
      GRAVITINO_ENTITY_STORE: relational
      GRAVITINO_ENTITY_RELATIONAL_JDBC_BACKEND_URL: jdbc:postgresql://postgres:5432/gravitino_db
      GRAVITINO_ENTITY_RELATIONAL_JDBC_BACKEND_DRIVER: org.postgresql.Driver
      GRAVITINO_ENTITY_RELATIONAL_JDBC_BACKEND_USER: gravitino
      GRAVITINO_ENTITY_RELATIONAL_JDBC_BACKEND_PASSWORD: gravitino123
      
      # JVM options
      GRAVITINO_JVM_OPTS: "-Xmx2g -Xms1g"
    depends_on:
      postgres:
        condition: service_healthy
      polaris:
        condition: service_started
      hive-metastore:
        condition: service_started
    volumes:
      - ./gravitino.conf:/gravitino/conf/gravitino.conf
      - gravitino-logs:/gravitino/logs
    networks:
      - poc-network

  # ============================================
  # SPARK (Compute Engine)
  # ============================================
  
  spark-master:
    image: apache/spark:3.5.1-scala2.12-java11-python3-ubuntu
    container_name: poc-spark-master
    ports:
      - "8080:8080"  # Spark Master Web UI
      - "7077:7077"  # Spark Master Port
      - "4040:4040"  # Spark Application UI
    environment:
      SPARK_MODE: master
      SPARK_MASTER_HOST: spark-master
      SPARK_MASTER_PORT: 7077
      
      # S3 Configuration
      AWS_ACCESS_KEY_ID: admin
      AWS_SECRET_ACCESS_KEY: password123
      AWS_REGION: us-east-1
    depends_on:
      - gravitino
    volumes:
      - ./spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./spark-jars:/opt/spark/jars-extra
      - ./notebooks:/opt/notebooks
      - ./scripts:/opt/scripts
    command: >
      bash -c "
      cp /opt/spark/jars-extra/* /opt/spark/jars/ 2>/dev/null || true &&
      /opt/spark/sbin/start-master.sh &&
      tail -f /opt/spark/logs/spark--org.apache.spark.deploy.master.Master-*.out
      "
    networks:
      - poc-network

  spark-worker:
    image: apache/spark:3.5.1-scala2.12-java11-python3-ubuntu
    container_name: poc-spark-worker
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2g
      
      # S3 Configuration
      AWS_ACCESS_KEY_ID: admin
      AWS_SECRET_ACCESS_KEY: password123
      AWS_REGION: us-east-1
    depends_on:
      - spark-master
    volumes:
      - ./spark-jars:/opt/spark/jars-extra
    command: >
      bash -c "
      cp /opt/spark/jars-extra/* /opt/spark/jars/ 2>/dev/null || true &&
      /opt/spark/sbin/start-worker.sh spark://spark-master:7077 &&
      tail -f /opt/spark/logs/spark--org.apache.spark.deploy.worker.Worker-*.out
      "
    networks:
      - poc-network

  # ============================================
  # JUPYTER NOTEBOOK (Interactive Development)
  # ============================================
  
  jupyter:
    image: jupyter/pyspark-notebook:spark-3.5.1
    container_name: poc-jupyter
    ports:
      - "8888:8888"  # Jupyter Notebook
    environment:
      JUPYTER_ENABLE_LAB: "yes"
      AWS_ACCESS_KEY_ID: admin
      AWS_SECRET_ACCESS_KEY: password123
      AWS_REGION: us-east-1
    depends_on:
      - spark-master
    volumes:
      - ./notebooks:/home/jovyan/work
      - ./spark-jars:/opt/spark/jars-extra
    command: >
      bash -c "
      cp /opt/spark/jars-extra/* /usr/local/spark/jars/ 2>/dev/null || true &&
      start-notebook.sh --NotebookApp.token='' --NotebookApp.password=''
      "
    networks:
      - poc-network

volumes:
  postgres-data:
  minio-data:
  gravitino-logs:

networks:
  poc-network:
    driver: bridge
