Workflow: Data Adjustment & Review Process

Data Ingestion: A tenant’s authorized user (or an automated upstream process) starts by submitting data to the system. For file-based sources, the Angular UI provides an upload interface; large files are handled by splitting or using a direct transfer to the staging storage rather than loading the entire file through the browser. For example, the UI might obtain a pre-signed URL to stream the file into an S3 staging bucket in chunks. Alternatively, if data comes from a database or API, a connector or ingestion microservice will write the incoming data into the staging area. In all cases, the incoming data is labeled with the tenant ID (if using a shared service) or just lands in the tenant-specific storage. The system may generate a dataset record (metadata entry) marking the upload event, which includes who uploaded it and basic stats (time, size, schema).

Validation & Profiling (Optional): Before presenting data for human review, the system can run automated validation rules or data profiling. For instance, it might check for schema mismatches, required fields, or detect anomalies. Any identified issues can be flagged in the metadata so the reviewers know what to pay attention to. This step could be expanded in future versions with advanced data quality checks or even AI/ML to flag likely errors, though initial version might rely on manual identification of issues.

Human Review via UI: The data is now ready for human review. A user with the “Data Steward” (or equivalent) role logs into the tenant’s UI. They retrieve the dataset from the staging storage (the UI calls the backend service, which queries the staging database or reads from the staging files). The UI might show a paginated table or form views. The user can edit values that are incorrect, add missing information, or mark records for exclusion as needed. Throughout this process, the Audit Log captures each change: e.g., “User X changed Field Y from ‘ABC’ to ‘XYZ’ on Record 123 at 10:30 AM”. The audit entries include user, timestamp, old value, new value, etc., to provide a complete change history for each record. If PII fields are present, the UI and backend enforce that only users with a privileged role can see or edit those fields – others might see a masked value (e.g., ********). This is configured via metadata: for example, the metadata store knows a column “AccountNumber” is sensitive, so the UI rendering layer applies a mask unless the user’s JWT token has a scope/role that allows full access. These precautions align with the principle of least privilege and ensure compliance by preventing unnecessary exposure of PII.

Submit for Approval: Once the data steward is satisfied with the corrections, they mark the dataset as “ready for approval” via the UI. At this point, no further edits are allowed until an approver reviews (the system could enforce a lock or snapshot of the dataset at this stage). A user with the “Approver” role (e.g., a supervisor or a compliance officer) is notified or can see pending approvals in the UI. The approver can then review a summary of changes (the system can present a diff of original vs edited values, using the audit log). This step ensures accountability – a second pair of eyes confirms that changes are accurate and policy-compliant. The approver can either sign off on the dataset or reject it (possibly with comments). If rejected, the dataset could be unlocked for further editing and the process repeats.

Publishing to Data Lake: Upon approval, the system triggers the publish step. The Data Adjustment Service will take the final dataset from staging and write it to the target data lake storage. If the target is an Iceberg table on S3, the service (which could be implemented in Java/Scala using the Iceberg API, or via a Spark job, etc.) will write the data file parts (e.g. Parquet files) to the appropriate S3 location and update the Iceberg table metadata to add a new snapshot. This operation is done in an atomic manner (Iceberg ensures either the new snapshot is visible in entirety or not at all), preserving data consistency. Because each tenant might have their own Iceberg table (or their own namespace in a shared catalog), this also naturally segments the data by tenant. In a Postgres target scenario, the service would upsert the records into the production schema, possibly tagging them with a batch ID or timestamp. After publishing, the data is officially in the data lake and available for analytics or downstream consumption. The system could notify other systems or users (e.g., send an event or email) that new clean data is available.

Archival and Cleanup: The original raw dataset and any previous versions need to be handled as per compliance and retention policies. Since financial and personal data is involved, nothing is simply deleted without consideration. One approach is leveraging Iceberg’s versioning for historical data – older snapshots remain accessible for audit queries or legal holds until explicitly purged. Additionally, the system could move the raw uploaded file to a secure archive storage (for example, an “archive” S3 bucket or a WORM (Write-Once-Read-Many) storage if regulatory rules require keeping original records unchanged). The audit log and metadata records are retained long-term as well. If GDPR’s “Right to be Forgotten” is invoked for any personal data, the system must be able to locate all relevant records (using metadata indices for PII) and delete or anonymize them in both staging and final storage, and even in archives if applicable. Designing for this might involve keeping a mapping of person identifiers to data records, so a deletion request can purge those across all layers. Finally, the staging area can be cleared of the active dataset (unless we choose to keep an editable copy for a time). Often, after successful publish, staging records are either deleted or marked as published, to avoid confusion and to free space.

Throughout this process, auditability and traceability are paramount. Audit logs provide a chronological record of all actions for each dataset and user, which is essential not only for internal accountability but also for external regulatory audits. In highly regulated environments, the ability to produce an audit trail demonstrating who changed what data and who approved it is a non-negotiable requirement. This audit trail, combined with Iceberg’s data version history and possibly a data lineage record (linking the dataset back to source and forward to the data lake), ensures end-to-end transparency.

Security and Compliance Measures

Tenant Isolation and Security: The choice to isolate each tenant’s application components and data at the namespace level provides strong security guarantees. According to best practices, each tenant having a separate namespace (and even separate underlying databases or storage containers) helps meet requirements for tenants with strict regulatory or data residency needs. For instance, if different banking clients require their data be kept in specific regions or using their own encryption keys, the architecture can accommodate that (by deploying their instance in a particular location and configuring their storage accordingly). Kubernetes network policies are implemented to block cross-namespace traffic, so even if one tenant’s app tries to call an internal service, it cannot reach another tenant’s services or database. Additionally, sensitive configuration like database credentials, API keys, or encryption keys are managed per-tenant (e.g. stored in Kubernetes Secrets within that tenant’s namespace). This siloed model greatly reduces the risk of data leakage between tenants and aligns with a zero-trust approach where each tenant environment is self-contained.

Authentication & Authorization: The platform uses OAuth2/OpenID Connect for user logins. Each tenant can integrate with an IdP of choice (for example, a cloud Cognito user pool, Keycloak realm, or the organization’s own OAuth server) to manage its users. Users authenticate and obtain JWT tokens, which the Angular UI and backend services validate on each request. The JWT carries the user’s roles/permissions (and implicitly the tenant identity, either by the domain they log into or an explicit tenant claim). An API Gateway or ingress controller can be configured to require a valid token for all requests. Within the app, fine-grained authorization checks ensure only specific roles can perform certain actions or view certain data. For example, even if a malicious user tried to call the backend API to fetch another tenant’s data, their token (containing a different tenant ID or being signed by a different tenant-specific issuer) would not be authorized to access it. The app should also implement role-based UI controls – e.g., hide or disable the “Approve” button if the user isn’t in an approver role. These measures enforce that only authorized personnel can amend and review PII data, as required. Moreover, all service-to-service communication and database connections occur over secure channels (TLS encryption in transit), and data at rest is encrypted (for cloud storage, S3 provides server-side encryption, or databases use TDE). Each tenant’s data can even be encrypted with tenant-specific keys if needed, adding another layer of separation (some regulations require each client’s data encrypted with keys only that client or a subset of admins can control).

PII Governance: The platform incorporates metadata-driven PII management. During initial data onboarding or schema configuration for a tenant, administrators can define which fields are sensitive. The system could integrate automated PII discovery tools in the future, but initially it relies on known schema definitions. Marking a field as PII triggers multiple controls: that field might be automatically masked in the UI for users without a special role, it might be omitted from being shown in change summaries to approvers if not necessary, and it ensures the audit log does not store actual PII values in plain text (to avoid creating another leakage point). Instead, audit logs might refer to record IDs and field names, with old/new values only visible to certain roles. Additionally, data minimization is practiced – only the necessary PII fields should be ingested and retained. For example, if a source feed has extra personal data that isn’t needed for the lake analysis, the ingestion process can drop or anonymize it upfront. The system also supports tokenization or masking: for highly sensitive identifiers, instead of storing actual values even in staging, the service could replace them with tokens or hashes and store the mapping in a secure vault. This way, analysts working downstream only deal with pseudonymized data, and only authorized systems can re-identify if absolutely required. All these practices ensure compliance with GDPR’s requirements (like protecting personal data, enabling deletion, etc.) and with financial data handling standards. For instance, GDPR emphasizes data protection by design (which we achieve via encryption, access control, and audit logging) and accountability (achieved via our comprehensive audit trails).

Auditing and Logging: As noted, every component generates logs for actions and errors. The audit log is a first-class component – it feeds a secure log store that only privileged users can query. The audit records include what changed, who did it, and when, and can be signed or write-once to prevent tampering. Regular reviews of audit logs can be done to detect any unauthorized activity. In addition to data change logs, system-level audits (user login attempts, permission changes, configuration changes to the system) are also recorded. This aligns with standard practices that audit trails are crucial for detecting security violations, tracing issues, and proving compliance. In case of any incident (say a data discrepancy or a suspected unauthorized change), the firm can rely on these logs to quickly pinpoint the source and take corrective action. The platform’s design thus inherently supports operational integrity and compliance verification through its logging.

Deployment and Technology Stack

Kubernetes & Multi-Cloud: The application is built to be cloud-agnostic. Kubernetes provides the common operating layer whether deployed on AWS (EKS) or on-prem (self-managed K8s or OpenShift, etc.). Each tenant’s namespace contains the necessary pods (deployments for the UI, the backend service, possibly a database instance or a connection to one, etc.). For AWS deployments, a tenant’s namespace might access cloud resources like S3 buckets that are scoped to that tenant. For on-prem or hybrid scenarios, one could use S3-compatible storage (like MinIO) or route to an on-prem data lake. The architecture supports hybrid deployment in that some tenants could even be served from an on-prem cluster while others are on cloud, if needed – since each is an isolated unit, they just have to meet the same interface/contract. Containerization also eases configurability: each tenant’s environment can load a config specific to that tenant (e.g. which data source types are enabled, what the data schema looks like, what roles exist and their permissions, etc.). This satisfies the requirement to be as configurable and generalized as possible, rather than hard-coding logic per tenant. New tenants can be onboarded by deploying a new instance (a process that could be automated with scripts or Helm charts). In a future Version 2 of the platform, a more dynamic or centralized multi-tenant management could be introduced – for example, a single control-plane service to manage multiple tenant namespaces, or even a pooled model where core services are shared but data is partitioned by tenant ID. For now, the silo model (one environment per tenant) maximizes security and simplicity, which is appropriate given the strict compliance needs (regulations often favor clear physical segregation of different clients’ data).

Application Layers: The frontend is an Angular application (latest stable Angular framework) which provides a rich interactive experience. It communicates with the backend via secure REST (or GraphQL) APIs. The backend can be implemented in a language like Java (Spring Boot), Node.js (Express/Nest), or Python (FastAPI/Django) – what matters is that it is a stateless service (for scalability) that encapsulates the business logic for data validation, committing edits, interfacing with storage, and orchestrating the workflow state machine (from upload to approved). The backend would interface with the staging database if using one (e.g., executing SQL on Postgres) and with the data lake (possibly using an SDK or JDBC to write to Iceberg tables; for example, one could use Apache Iceberg’s libraries to create a table snapshot). If using an object store for staging, the backend might use a distributed query engine (like Apache Spark or Trino) to query data for the UI – but given the data sizes (a few GB per dataset) and the need for edits, a simpler approach is likely storing staging data in a SQL database. Each tenant could have a dedicated Postgres schema or even an isolated Postgres instance/pod (depending on scale and preference for isolation). The identity provider component might be external (e.g., an OAuth service provided to all tenants or one per tenant). In a minimal deployment, a simple OAuth2 server (like Keycloak) can be run per tenant to handle user auth without federation, since the app is deployed in that tenant’s context and can rely on namespace-level network isolation as noted.

Compliance Tooling: In support of metadata and PII management, the architecture could integrate tools like Apache Atlas or AWS Glue Data Catalog for metadata governance, and maybe use something like HashiCorp Vault for managing encryption keys or tokenization secrets. However, these are implementation details that can be added as needed. At the very least, the platform will maintain a metadata configuration (perhaps in a relational table or a YAML config) that lists which fields in the dataset are sensitive, and what compliance actions are tied to them (masking, encryption, etc.). Logging and monitoring components (ELK stack or cloud monitor services) will be set up to track application health, usage per tenant, and to alert on any suspicious activities (for instance, if someone tries to access unauthorized data, it should be logged as a security event).

In summary, this architecture provides a robust, secure, and flexible framework for multi-tenant data adjustment and review. By isolating tenants at the Kubernetes namespace level and storage level, it ensures strong security and compliance separation. The inclusion of a human review loop with audit logging guarantees accountability and accuracy for the critical 10% of data that requires manual intervention, while still automating the movement of approved data into the governed data lake. The design addresses practical challenges like large file handling (via chunked/multipart ingestion) and enforces data governance measures (role-based access to PII, encryption, retention policies) at every stage, thereby aligning with GDPR and financial regulatory requirements. With a modern Angular UI and scalable cloud-native backend, the solution is user-friendly yet enterprise-grade. It can run on AWS, on-premises, or hybrid, providing the needed flexibility for different deployment scenarios. Going forward, this foundation can be extended (e.g., more automation in data validation, a centralized tenant management plane in version 2, or integration with enterprise data catalogs), but even in its initial form it meets the core needs of secure, audited, multi-tenant data preparation for a data lake.
